{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3e22df",
   "metadata": {},
   "source": [
    "# Unit tag and Span cleaning\n",
    "\n",
    "This script combines two steps in the OCOD processing pipeline.\n",
    "\n",
    "* unit tagging\n",
    "* Removing overlapping spans\n",
    "\n",
    "These two processes are separated by the weak labelling in humanloop but as they are relatively simple they are included in a single script\n",
    "\n",
    "* **Raw CSV loaded and lightly processed. Output**: two column csv columns, property address, unit tag\n",
    "* Data labelled in programmatic. Output: json file of entities.\n",
    "* **Data programmatic output json cleaned ordered and overlaps removed**. Output: json file\n",
    "* Clean json converted to dataframe and multi-addresses expanded. Output: CSV\n",
    "* Count and locate addresses\n",
    "* Create address matcher and match businesses\n",
    "* Classify address types\n",
    "\n",
    "## Unit tagging\n",
    "\n",
    "This park of the pipeline adds in a binary value indicating whether the line contains flats/units/stores etc which are likely to have unit level ID. This is important as such addresses are likely to have a unit ID AND an street number and as such need to be treated with care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320c86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from prep_helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b9655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-63d38e693173>:1: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv('./data/' +\n",
      "<ipython-input-2-63d38e693173>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ocod_data.property_address = ocod_data.property_address.str.replace('\\s{2,}', r' ')\n",
      "<ipython-input-2-63d38e693173>:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
      "<ipython-input-2-63d38e693173>:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ocod_data =  pd.read_csv('./data/' +\n",
    "                    'OCOD_FULL_2022_02.csv',\n",
    "                   encoding_errors= 'ignore').rename(columns = lambda x: x.lower().replace(\" \", \"_\"))\n",
    "ocod_data['postcode'] = ocod_data['postcode'].str.upper()\n",
    "#empty addresses cannot be used. however there are only three so not a problem\n",
    "ocod_data = ocod_data.dropna(subset = 'property_address')\n",
    "ocod_data.reset_index(inplace = True, drop = True)\n",
    "ocod_data['property_address'] = ocod_data['property_address'].str.lower()\n",
    "\n",
    "#ensure there is a space after commas\n",
    "#This is because some numbers are are written as 1,2,3,4,5 which causes issues during tokenisation\n",
    "ocod_data.property_address = ocod_data.property_address.str.replace(',', r', ')\n",
    "#remove multiple spaces\n",
    "ocod_data.property_address = ocod_data.property_address.str.replace('\\s{2,}', r' ')\n",
    "\n",
    "\n",
    "#different words associated with unit ID's\n",
    "flatregex = r\"(flat|apartment|penthouse|unit)\" #unit|store|storage these a\n",
    "\n",
    "#This is not an exhaustive list of road names but it covers about 80% of all road types in the VOA business register.\n",
    "#The cardinal directions are includted as an option as they can appear after the road type. However they serve no real purpose in this particular regex and are \n",
    "#included for completness\n",
    "road_regex  = r\"((road|street|lane|way|gate|avenue|close|drive|hill|place|terrace|crescent|gardens|square|walk|grove|mews|row|view|boulevard|pleasant|vale|yard|chase|rise|green|passage|friars|viaduct|promenade|end|ridge|embankment|villas|circus))\\b( east| west| north| south)?\"\n",
    "#These names may be followed by a road type e.g. Earls court road. A negative lookahead is used to prevent these roads being tagged as units.\n",
    "flatregex2 = r\"(mansions|villa|court)\\b(?!(\\s\"+road_regex+\"))\"\n",
    "\n",
    "#flat_tag is used for legacy reasons but refers to sub-units in general\n",
    "ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
    "\n",
    "ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n",
    "\n",
    "#typo in the data leads to a large number of fake flats\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"stanley court \", \"stanley court, \")\n",
    "#This typo leads to some rather silly addresses\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"100-1124\", \"100-112\")\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"40a, 40, 40Â¨, 42, 44\", \"40a, 40, 40, 42, 44\")\n",
    "\n",
    "\n",
    "#only two columns are needed for the humanloop labelling process\n",
    "ocod_data[['property_address', 'flat_tag', 'commercial_park_tag', 'title_number']].rename(columns = {'property_address':'text'}).to_csv('./data/property_address_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503ca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the index for the ground truth\n",
    "random.seed(2017)\n",
    "test_set = random.sample([*range(0, ocod_data.shape[0])], 1000) \n",
    "\n",
    "test_set = ocod_data.loc[test_set, 'title_number'].reset_index().rename(columns = {'index':'datapoint_id'})\n",
    "\n",
    "test_set.to_csv('./data/test_set_indices_space_cleaned_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b8c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Belatedly create dev set\n",
    "#This also needs to be manually labelled and so is also pretty small.\n",
    "\n",
    "dev_set_all = ocod_data.loc[~ocod_data.title_number.isin(test_set.title_number),:]\n",
    "random.seed(2017)\n",
    "dev_set = random.sample(dev_set_all.title_number.to_list(), 2000)\n",
    "\n",
    "dev_set = dev_set_all.loc[dev_set_all.title_number.isin(dev_set), 'title_number'].reset_index().rename(columns = {'index':'datapoint_id'})\n",
    "\n",
    "dev_set.to_csv('./data/dev_set.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd0d5d",
   "metadata": {},
   "source": [
    "## Labelling in Humanloop\n",
    "\n",
    "This part of process uses the humanloop programmatic app and is an external process. Once the labelling step is complete the process outputs a json file containing the labels and spans, this is then cleaned in the next step.\n",
    "\n",
    "## Removing overlapping spans\n",
    "\n",
    "During the humanloop tagging process the rules may result in the same words being tagged as part of multiple spans, this often occures for road names made up of multiple parts \n",
    "e.g. Canberra Crescent Gardens may be tagges as Canberra Cresecent and Canberra Crescent Gardens. The overlaps need to be removed before further prcoessing.\n",
    "For simplicity the largest span of any two overlapping spans is kept and the smaller of the two is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c6012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These libraries are specific to this part of the process\n",
    "import json\n",
    "import requests \n",
    "import config #contains hidden api key\n",
    "import operator #used for sorting the label dictionaries by start point. This is the basis for removing overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae4faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f =open(\"./data/test.json\")  #aggregate and download button\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "#this makes a list of all the observation rows. These refer to the row of the orginal observation text and so can be linked back to the original OCOD dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589b107",
   "metadata": {},
   "source": [
    "## Clean the results\n",
    "\n",
    "Take the non-denoised results and keep only the longest of any overlapping elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4270572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 5000\n",
      "count = 10000\n",
      "count = 15000\n",
      "count = 20000\n",
      "count = 25000\n",
      "count = 30000\n",
      "count = 35000\n",
      "count = 40000\n",
      "count = 45000\n",
      "count = 50000\n",
      "count = 55000\n",
      "count = 60000\n",
      "count = 65000\n",
      "count = 70000\n",
      "count = 75000\n",
      "count = 80000\n",
      "count = 85000\n",
      "count = 90000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_labels_dict = clean_programmatic_for_humanloop(data, ocod_data)\n",
    "    \n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('./data/full_dataset_no_overlaps.json', 'w') as f:\n",
    "    json.dump(data_labels_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e7f63",
   "metadata": {},
   "source": [
    "### Uploading to humanloop cloud\n",
    "\n",
    "This allows a sample of the data to be uploaded to the humanloop cloud so that an example model can be made.\n",
    "The model provides another way to check the quality of the weak labelling. However, only 10k obersvations can be uploaded, as such a sub-sample is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63fffc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "f =open('./data/full_dataset_no_overlaps.json')  \n",
    "data_labels_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa90b7",
   "metadata": {},
   "source": [
    "### Create new project with unlabelled data\n",
    "\n",
    "This creates the json and uploads it as a new project to human loop. The data can then be hand labelled to create a ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d45c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jason_test_data ={\n",
    "     \"name\": \"test_set_24_04_22\",\n",
    "     \"description\": \"the ground truth test set for labelling\",\n",
    "     \"fields\": [\n",
    "         {\"name\": \"text\", \n",
    "          \"data_type\": \"text\"\n",
    "         },\n",
    "         {\"name\": \"labels\", \n",
    "          \"data_type\": \"character_offsets\"},\n",
    "         {\"name\": \"datapoint_id\", \n",
    "          \"data_type\": \"text\"\n",
    "         }\n",
    "     ],\n",
    "     \"data\": [data_labels_dict[x] for x in test_set.loc[:,'datapoint_id']]#, #upload only data from the test set\n",
    "}\n",
    "upload_to_human_loop(jason_data = jason_test_data, config, project_owner = \"jonathan.s.bourne@gmail.com\" )\n",
    "del jason_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "550b60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jason_dev_data ={\n",
    "     \"name\": \"dev_set_13_05_22\",\n",
    "     \"description\": \"the ground truth test set for labelling\",\n",
    "     \"fields\": [\n",
    "         {\"name\": \"text\", \n",
    "          \"data_type\": \"text\"\n",
    "         },\n",
    "         {\"name\": \"labels\", \n",
    "          \"data_type\": \"character_offsets\"},\n",
    "         {\"name\": \"datapoint_id\", \n",
    "          \"data_type\": \"text\"\n",
    "         }\n",
    "     ],\n",
    "     \"data\": [data_labels_dict[x] for x in dev_set.loc[:,'datapoint_id']]#, #upload only data from the test set\n",
    "}\n",
    "\n",
    "\n",
    "upload_to_human_loop(jason_data = jason_dev_data, config, project_owner = \"jonathan.s.bourne@gmail.com\" )\n",
    "\n",
    "del jason_dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e2eec",
   "metadata": {},
   "source": [
    "# Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f4bf2",
   "metadata": {},
   "source": [
    "## create spacy format\n",
    "\n",
    "The below chunk creates the format using the output of programmatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e06388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "aggregateResults\n",
      "removing overlapping spans from training data\n",
      "count = 5000\n",
      "count = 10000\n",
      "count = 15000\n",
      "count = 20000\n",
      "count = 25000\n",
      "count = 30000\n",
      "count = 35000\n",
      "count = 40000\n",
      "count = 45000\n",
      "count = 50000\n",
      "count = 55000\n",
      "count = 60000\n",
      "count = 65000\n",
      "count = 70000\n",
      "count = 75000\n",
      "count = 80000\n",
      "count = 85000\n",
      "count = 90000\n",
      "removing overlapping spans from dev data\n",
      "creating training DocBin\n",
      "creating dev DocBin\n"
     ]
    }
   ],
   "source": [
    "from prep_helper_functions import *\n",
    "\n",
    "training_data, dev_data = create_spacy_training_set(programmatic_data_path = \"./data/test.json\",\n",
    "                              dev_set_path = './data/ground_truth_dev_set_labels.json',\n",
    "                              hmm_denoising = True,\n",
    "                              alignment_mode_type = \"expand\",\n",
    "                              save_folder = '/tf/enhance_ocod/'+\"data/spacy_data/training/data_hmm_24_05_22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436272c",
   "metadata": {},
   "source": [
    "## spacy training data debugging\n",
    "\n",
    "The below chunks help debug the creation of the spacy training data.\n",
    "\n",
    "Errors are usually caused by tokenization issues. Several of this issues have been solved by changing the 'infixes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "537d2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datapoint_id': 5001,\n",
       " 'text': 'units d.01 and d.02, trathen square, london (se10 0et)',\n",
       " 'entities': [[0, 5, 'unit_type'],\n",
       "  [6, 7, 'unit_id'],\n",
       "  [8, 10, 'unit_id'],\n",
       "  [21, 35, 'street_name'],\n",
       "  [37, 43, 'city'],\n",
       "  [45, 53, 'postcode']]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data =  train_set\n",
    "training_data[4908]\n",
    "#The rule is splitting the unit id on the . but why? it shouldnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd6f5fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignment mode expand\n",
      "units\n",
      "d.01\n",
      "d.01\n",
      "trathen square\n",
      "london\n",
      "se10 0et\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 1 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-13ce0ac3fc66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 1 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "#nlp = spacy.blank(\"en\")\n",
    "training_data = train_set\n",
    "# the DocBin will store the example documents\n",
    "print('alignment mode '+alignment_mode_type)\n",
    "db = DocBin()\n",
    "for i in range(i,i+1):\n",
    "    current_set = training_data[i]\n",
    "    doc = nlp(current_set['text'])\n",
    "    ents = []\n",
    "    for start, end, label in current_set['entities']:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode = alignment_mode_type )\n",
    "        print(span)\n",
    "        ents.append(span)\n",
    "    doc.ents = ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85de41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2mâ Saved config\u001b[0m\r\n",
      "/tf/empty_homes_london/config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /tf/empty_homes_london/base_config.cfg /tf/empty_homes_london/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code to run the training model\n",
    "#!python -m spacy train config.cfg --paths.train /tf/data/spacy_data/train.spacy --paths.dev /tf/data/spacy_data/dev.spacy --output /tf/data/spacy_data/ --gpu-id 1\n",
    "#!python -m spacy train config.cfg --paths.train /home/jonno/data/spacy_data/train.spacy --paths.dev /home/jonno/data/spacy_data/dev.spacy --output /home/jonno/data/spacy_data/cpu\n",
    "#python -m spacy train ./spacy_config_files/cpu_config.cfg --paths.train ./data/spacy_data/train.spacy --paths.dev ./data/spacy_data/dev.spacy --output ./data/spacy_data/cpu3\n",
    "\n",
    "#!python -m spacy train ./spacy_config_files/cpu_config.cfg --paths.train ./data/spacy_data/training/data_hmm_24_05_22/train.spacy --paths.dev ./data/spacy_data/training/data_hmm_24_05_22/dev.spacy --output ./data/spacy_data/training/data_hmm_24_05_22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc898c8",
   "metadata": {},
   "source": [
    "\n",
    "# Predicting using spacy\n",
    "\n",
    "The below chunks are used to predict label from data using spacy. There are some issues that appear to be related to a recent update of CUDA which has caused a variety of problems. As such this part of the code is being kept separate and some of the code choices may look very strange. \n",
    "\n",
    "Interestingly the performance of the spaCy model is pretty much identical to the labels created using rules. This suggests that the labelling is probably very good and also that spacy is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92ef917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/empty_homes_london\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/empty_homes_london/address_parsing_helper_functions.py:463: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv(file_path,\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "from address_parsing_helper_functions import load_and_prep_OCOD_data\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "#spacy.require_gpu()\n",
    "#spacy.prefer_gpu()\n",
    "\n",
    "nlp1 = spacy.load(\"/tf/data/spacy_data/cpu/model-best\") \n",
    "\n",
    "ocod_data = load_and_prep_OCOD_data('/tf/data/' +'OCOD_FULL_2022_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0ea683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3141946792602539\n"
     ]
    }
   ],
   "source": [
    "#transformer takes about 85 minutes with cpu and 2.35 minutes with GPU\n",
    "#However if there is a recent update to pytorch there can be porblems with the GPU inference https://github.com/explosion/spaCy/issues/8229\n",
    "#In addition if nvidia updates something Docker can have difficult to resolve bugs\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "#with torch.no_grad():\n",
    "spacy_docs_list = list(nlp1.pipe(ocod_data.loc[1:100,'property_address']))\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)\n",
    "\n",
    "#This runtime comparison of spacy using cpu and gpu. \n",
    "#GPU about 5-6 times faster for inference on a transformer\n",
    "#https://github.com/BlueBrain/Search/issues/337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc5fe894",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_path = \"/tf/data/spacy_data/cpu/model-best\"\n",
    "\n",
    "def spacy_pred_fn(spacy_model_path, ocod_data, print_every =1000):\n",
    "    nlp1 = spacy.load(spacy_model_path) \n",
    "\n",
    "    ocod_context = [(ocod_data.loc[x,'property_address'], {'datapoint_id':x}) for x in range(0,ocod_data.shape[0])]\n",
    "    i = 0\n",
    "    all_entities_json = []        \n",
    "    for doc, context in list(nlp1.pipe(ocod_context[0:1000], as_tuples = True)):\n",
    "\n",
    "        #This doesn't print as it is a stream not a conventional loop\n",
    "        #if i%print_every==0: print(\"doc \", i, \" of \"+ str(ocod_data.shape[0]))\n",
    "        #i = i+1\n",
    "\n",
    "        temp = doc.to_json()\n",
    "        temp.pop('tokens')\n",
    "        temp.update({'datapoint_id':context['datapoint_id']})\n",
    "        all_entities_json = spacy_docs_list + [temp]\n",
    "\n",
    "    all_entities = pd.json_normalize(all_entities_json, record_path = \"ents\", meta= ['text', 'datapoint_id'])\n",
    "    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaa18da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy_pred_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a0a1e53f220e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_pred_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocod_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy_pred_fn' is not defined"
     ]
    }
   ],
   "source": [
    "test = spacy_pred_fn(spacy_model_path, ocod_data, print_every =1000)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f60fe",
   "metadata": {},
   "source": [
    "### This is designed to avoid the prediction crash that occurs possibly due to the nvidia upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5527173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin getting labels\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "target_data = ocod_data.loc[:,'property_address']\n",
    "lower = [0, 20000,40000,60000,80000]\n",
    "upper = [20000,40000,60000,80000,len(target_data)]\n",
    "\n",
    "ocod_context = [(ocod_data.loc[x,'property_address'], {'datapoint_id':x}) for x in range(0,ocod_data.shape[0])]\n",
    "\n",
    "print('begin getting labels')\n",
    "\n",
    "for x in range(0,5):\n",
    "    print(x)\n",
    "    spacy_docs_list = []        \n",
    "    for doc, context in list(nlp1.pipe(ocod_context[lower[x]:upper[x]], as_tuples = True)):\n",
    "        temp = doc.to_json()\n",
    "        temp.pop('tokens')\n",
    "        temp.update({'datapoint_id':context['datapoint_id']})\n",
    "        spacy_docs_list = spacy_docs_list + [temp]\n",
    "        \n",
    "    file_name = '/home/jonno/data/spacy_pred_labels' + str(x) + '.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(spacy_docs_list, f)\n",
    "        \n",
    "all_entities_json = []\n",
    "for x in range(0,5):\n",
    "    print(x)\n",
    "    file_name = '/home/jonno/data/spacy_pred_labels' + str(x) + '.json'\n",
    "    f =open(file_name)  #aggregate and download button\n",
    "    all_entities_json = all_entities_json  + json.load(f)\n",
    "        \n",
    "all_entities = pd.json_normalize(all_entities_json, record_path = \"ents\", meta= ['text', 'datapoint_id'])\n",
    "\n",
    "all_entities.to_csv('/home/jonno/data/spacy_preds_normalised.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56a1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jonno/data/spacy_pred_labels.json', 'w') as f:\n",
    "    json.dump(spacy_labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
