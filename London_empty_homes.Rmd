---
title: "Untitled"
author: "Jonathan Bourne"
date: "07/09/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---


I should check spatial auto-corellation for the key variables. They should have spatial elements, but checking and commenting still has value.


THe prices should be updated and the change in price calculated over the brexit period and probably the last crash, this will let us see if areas with high numbers of empty property have higher volatility

#Fascinating law change in minneapolis. 
Minneapolis Saw That NIMBYism Has Victims
Single-family zoning hurts a lot of people. In Minnesotaâ€™s largest city, reformers put them front and center.
Richard D. Kahlenberg
Senior fellow at The Century Foundation 

https://www.theatlantic.com/ideas/archive/2019/10/how-minneapolis-defeated-nimbyism/600601/?utm_campaign=the-atlantic&utm_medium=social&utm_term=2019-10-24T10%3A00%3A24&utm_source=facebook&utm_content=edit-promo&fbclid=IwAR30lz6cF4-vXkNDRBvJ_ZWDcoVJp8JjJMOfXc9JnQBYp9jz2Pfp8d0Hudk

non paywall article
https://www.southwestjournal.com/news/2019/10/triplex-change-slated-for-2040s-first-day/




#NExt MOOOVE

##missing sales data
I need to create a function that fills in missing geographies with the nearest non-missing data. This could be done using either and ego network or LSOA -> MSOA ->LAD data. ego network is better LAD is easier.
The first step is to workout how many geographies actually have missing data. if it is 0 at MSOA level then I can just do that

The best idea to solve this is to simply append the filler data to the price df where the geogrphy code is that which needs to be added. This requires minimum complexity during the sampling process, which will keep it fast and simple.


##aggregation. 

Aggregating the total values does not seem to be working well, the weights seem all over the place, this needs to be fixed.

```{r}

library(sf)

SubCode <- "~/Dropbox/SSE/Empty Homes/EmptyHomesCode/SubCode"
setwd(SubCode)
source("Setup.R")

library(data.table)

setwd(Functions)
list.files() %>% map(source)

basewd_London <- "/home/jonno/Dropbox/SSE/London_Empty_Homes"
London_Data <- file.path(basewd_London, "Data")
Borough_maps <- file.path(basewd_London, "Borough maps")
Borough_maps_split <- file.path(basewd_London, "Borough maps_split")
lsoa_shapefile_path <- "~/Dropbox/SSE/Empty Homes/ShapeFiles/Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales"

#create all necessary folders
c(Borough_maps, Borough_maps_split) %>% walk(~{
  if(!dir.exists(.x)){dir.create(.x)}
})


list.files("/home/jonno/empty_homes_london/functions", full.names = T) %>%
  walk(~source(.x))

Figures <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Figures" #file.path(basewd, "Figures")
TexTables <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Tables"
suppressMessages(source(file.path(CommonCode, "AuxdataLoad.R")))
```

```{r}
print("Region London")

#The original London data set
source(file.path(CommonCode, "LondonFullProcesss.R"))

#The remaining 3 boroughs

#Havering

 HaveringLONDATA <- read_xlsx("HaveringDiscountsLSOA.xlsx" ) %>%  
   mutate(LSOA_CODE = WARD_CODE) %>% select(Discount_Class, LSOA_CODE , everything()) %>%
   {.[1:4]} %>%
   StructureData(2:3) %>%
   mutate(MSOALowuseClass = fct_explicit_na(MSOALowuseClass, na_level = "0-10"))

#Tower Hamlets
 
 TowerHamletsLONDATA <- read_xlsx("Tower HamletsDiscountsLSOA - complete v1.xlsx" ) %>%  
#   mutate(LSOA_CODE = WARD_CODE) %>% select(Discount_Class, LSOA_CODE , everything()) %>%
   mutate(spare = NA) %>% 
   StructureData(2:8)

#Harrow
 HarrowLONDATA <- read_xlsx("3527395 HarrowdiscountsLSOA.xlsx" )[1:4] %>%  
   StructureData(2:6) 
 
 
#
# Scrub the data to remove Cross overs with other LADs
#

# Combine Lads into a single df
#
#

DATAdf <- ls(pattern = "DATA$") %>%
  map_df(~{
    get(.x) %>%
      select(LSOA11CD, LowUse:LowuseClass) %>%
      group_by(LAD11CD) %>%
      mutate(LAD11CDCounts = n()) %>%
      arrange(-LAD11CDCounts) %>%
      ungroup %>%
      mutate(LAD11CD = first(LAD11CD),
             LAD11NM = first(LAD11NM)) #This allows me to keep tabs on the missing empty homes. this was caused by postcode problems
  
      }) %>%
  select(-LAD11CDCounts) %>%   mutate(LowUsePerc = ifelse(is.na(LowUsePerc), 0 , LowUsePerc),
    percentile_LowUsePerc = percent_rank(LowUsePerc),
    percentile_MeanPrice = percent_rank(MeanPrice),
    LSOAtype = case_when(
       percentile_MeanPrice >0.5 & percentile_LowUsePerc> 0.5  ~"HVHL",
        percentile_MeanPrice <0.5 & percentile_LowUsePerc> 0.5  ~"LVHL",
        percentile_MeanPrice >0.5 & percentile_LowUsePerc< 0.5  ~"HVLL",
       TRUE ~"LVLL"
    )) 

rm(list = ls(pattern = "DATA$"))


test <- DATAdf %>% ungroup %>%
  filter(!is.na(LowuseClass)) %>%
  arrange(-ValLow) %>%
  mutate(cumsum_val = cumsum(ValLow),
         cum_perc = cumsum_val/sum(ValLow),
         cum_count = 1:n()/nrow(.),
         pareto = cum_perc<0.8) 

setwd(basewd)
write.csv(DATAdf, "London_data.csv")

```


##Deprivation

The deprivation data comes from the ONS and is available from https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019

lower scores are MORE deprived, higher scores are LESS deprived

NOTE: this data includes only England

```{r}
deprivation_df <- read_excel(file.path(London_Data , "File_1_-_IMD2019_Index_of_Multiple_Deprivation.xlsx"),  
                             sheet = "IMD2019",
                   .name_repair = "universal")  %>%
  set_names(., nm = tolower(names(.))) %>%
  select(lsoa.code = "lsoa.code..2011.", imd_rank = "index.of.multiple.deprivation..imd..rank")


```






#airbnb data

```{r}

airbnb_df <- process_airnbnb_data(LSOAshapedata, CorePstCd, airbnb_csv = file.path(London_Data, "listings.csv"))

```

#Bind key variables together

This chunk takes the total homes, the empty homes, the offshore homes, the Airbnb homes and the multiple indices of deprivation and binds them together by LSOA.

```{r}
#The data is summarised due to some cross over between local authorities
#This means the LSOA price averages need to be calculated again
all_variables <- DATAdf %>% select(LSOA11CD, homes = Homes, low_use = LowUse) %>%
  filter(complete.cases(.)) %>%
  group_by(LSOA11CD) %>%
  summarise(across(.fns = sum)) %>%
  left_join(CorePstCd %>% select(LSOA11CD, MSOA11CD, LAD11CD, Region) %>% distinct()) %>%
  left_join(airbnb_df) %>%
  left_join(deprivation_df %>% rename(LSOA11CD = lsoa.code )) %>%#
  #replaces NA's caused by zero entries with 0
mutate_if(is.numeric,coalesce,0) %>%
  #remove any outside London straggelers
  filter(Region == "E12000007")


all_variables_lad <- all_variables %>%
  group_by(LAD11CD) %>%
  summarise(homes = sum(homes),
            low_use = sum(low_use),
            airbnb = sum(airbnb)) %>%
  left_join(CorePstCd %>% select(LAD11CD, LAD11NM) %>% distinct())


#almost no corellation between emptyness and deprivation
cor(all_variables$low_use, all_variables$imd_rank, method = "kendall")

cor(all_variables$low_use, all_variables$mean_price, method = "kendall")
test <- CorePstCd %>% select(LSOA11CD, MSOA11CD, LAD11CD) %>% distinct()

```



#Price adaptor

There is something weird with the prices when they are missing from the geographies

```{r}

price_test <- prices %>%
  filter(LAD11CD=="E09000020") %>%
  group_by(MSOA11CD) %>%
  summarise(counts = n())

length(unique(price_test$LSOA11CD))


test[!(unique(test$LSOA11CD) %in% unique(price_test$LSOA11CD)),]

length(unique(df$MSOA11CD))
test <- DATAdf  %>%
  filter(LAD11CD=="E09000020")

```



#sample empty non-empty prices

```{r}
start_time <- Sys.time()
test <- all_variables %>%
  mutate(non_target = homes-airbnb) %>%
  monte_carlo_stratified_dataset(df = ., variables  = c("non_target", "airbnb"), prices, size= 100, geography_name = "MSOA11CD")
end_time <- Sys.time()


df <-  all_variables %>%
  mutate(non_target = homes-airbnb)
print(end_time-start_time)


all_vars_monte <- c("low_use", "airbnb") %>% map(~{
  all_variables %>%
    mutate(non_target = homes-.data[[.x]]) %>%
    monte_carlo_stratified_dataset(.,c("non_target", .x), prices, 501, geography_name = "MSOA11CD")
})



all_vars_monte_df <- all_vars_monte %>%
  map_df(~{
    
.x[[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name !="non_target")
    
  }) %>%
  bind_rows(all_vars_monte[[1]][[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name =="non_target"))

all_vars_monte_df %>%
  mutate(LAD2 = as.factor(LAD11CD) %>% as.integer()) %>%
 # filter(LAD2 ==10) %>%
  ggplot(aes(x = value, colour = name)) + geom_density()

test <-DATAdf %>%
  select(LAD11CD, LAD11NM) %>%
  distinct()


all_vars_monte[[1]][[1]] %>%
  pivot_longer(., cols = 2:4)%>%
  filter(LAD11CD=="E09000020")   %>%
  ggplot(aes(x = value, colour = name)) + geom_density()


test <- all_vars_monte[[1]][[1]] %>%
  filter(LAD11CD=="E09000020") 


df <- DATAdf  %>%
  filter(LAD11CD=="E09000020") %>% 
        filter(!is.na(Homes))  %>%
        mutate(occupied = Homes- LowUse) %>%
        select(MSOA11CD, LSOA11CD, low_use = LowUse, occupied, LAD11CD) 

test3 <-   monte_carlo_stratified_dataset(df = df, variables = c("occupied", "low_use"), prices, size = 5, geography_name = "MSOA11CD")

.x <- "E09000020"

test4 <- test3$samples %>%
  mutate(total2 = (occupied* sum(df$occupied)+ low_use*sum(df$low_use))/(sum(df$occupied)+sum(df$low_use)))

test3$samples  %>%
  pivot_longer(., cols = 2:4)%>%
  filter(LAD11CD=="E09000020")   %>%
  ggplot(aes(x = value, colour = name)) + geom_density()
  


#There are significant differences so something is wrong
#The distributions should pretty much identical
 all_vars_monte[[1]][[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name =="non_target") %>%
  mutate(type = "low_use") %>%
bind_rows(all_vars_monte[[2]][[1]] %>%
  select(1, 3:5) %>%
  pivot_longer(., cols = 2:3) %>%
  filter(name =="non_target") %>%
    mutate(type = "aribnb")) %>%
   filter(LAD11CD ==  "E09000020") %>%
  ggplot(aes(x = value, colour = type)) + geom_density()
   


```


