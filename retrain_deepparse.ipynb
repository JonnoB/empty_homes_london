{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009d3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poutyne\n",
    "\n",
    "import os\n",
    "\n",
    "from deepparse import download_from_url\n",
    "from deepparse.dataset_container import PickleDatasetContainer\n",
    "from deepparse.parser import AddressParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e29ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's download the train and test data with \"new tags\" from the public repository.\n",
    "saving_dir = \"/tf/empty_homes_data/deepparse_data\"\n",
    "file_extension = \"p\"\n",
    "training_dataset_name = \"sample_incomplete_data\"\n",
    "test_dataset_name = \"test_sample_data\"\n",
    "download_from_url(training_dataset_name, saving_dir, file_extension=file_extension)\n",
    "download_from_url(test_dataset_name, saving_dir, file_extension=file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f571c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a training and test container.\n",
    "training_container = PickleDatasetContainer(os.path.join(saving_dir, training_dataset_name + \".\" + file_extension))\n",
    "test_container = PickleDatasetContainer(os.path.join(saving_dir, test_dataset_name + \".\" + file_extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd542e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bda1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings model\n",
      "downloading https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs100000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1965223/1965223 [00:00<00:00, 5283216.75B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs100000.d300.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112202964/112202964 [00:13<00:00, 8608446.26B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the weights for the network bpemb_attention.\n"
     ]
    }
   ],
   "source": [
    "# We will retrain the fasttext attention version of our pretrained model.\n",
    "model = \"bpemb\"\n",
    "address_parser = AddressParser(model_type=model, device=0, attention_mechanism=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b84e1066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PickleDatasetContainer in module deepparse.dataset_container.dataset_container:\n",
      "\n",
      "class PickleDatasetContainer(DatasetContainer)\n",
      " |  PickleDatasetContainer(*args, **kwds)\n",
      " |  \n",
      " |  Pickle dataset container that imports a list of addresses in pickle format and does some validation on it.\n",
      " |  \n",
      " |  The dataset needs to be a list of tuples where the first element of each tuple is the address (a string),\n",
      " |  and the second is a list of the expected tag to predict (e.g. ``[('an address', ['a_tag', 'another_tag']), ...]``).\n",
      " |  The len of the tags needs to be the same as the len of the address when whitespace split.\n",
      " |  \n",
      " |  For a training container, the validation tests applied on the dataset are the following:\n",
      " |  \n",
      " |      - all addresses are not empty,\n",
      " |      - all addresses are not whitespace string,\n",
      " |      - all tags are not empty, if data is a list of tuple (``[('an address', ['a_tag', 'another_tag']), ...]``), and\n",
      " |      - if the addresses (whitespace-split) are the same length as their respective tags list.\n",
      " |  \n",
      " |  While for a predict container (unknown prediction tag), the validation tests applied on the dataset are the\n",
      " |  following:\n",
      " |  \n",
      " |      - all addresses are not empty,\n",
      " |      - all addresses are not whitespace string.\n",
      " |  \n",
      " |  Args:\n",
      " |      data_path (str): The path to the pickle dataset file.\n",
      " |      is_training_container (bool): Either or not, the dataset container is a training container. This will determine\n",
      " |          the dataset validation test we apply to the dataset. That is, a predict dataset doesn't include tags.\n",
      " |          The default value is true.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PickleDatasetContainer\n",
      " |      DatasetContainer\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data_path: str, is_training_container: bool = True) -> None\n",
      " |      Need to be defined by the child class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DatasetContainer:\n",
      " |  \n",
      " |  __getitem__(self, idx: Union[int, slice])\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  is_a_train_container(self) -> bool\n",
      " |  \n",
      " |  validate_dataset(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  __getattr__(self, attribute_name)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing=False) from abc.ABCMeta\n",
      " |  \n",
      " |  register_function(function_name, function) from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __annotations__ = {'functions': typing.Dict[str, typing.Callable]}\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  functions = {'concat': functools.partial(<function Dataset.register_da...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PickleDatasetContainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8926846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's retrain for 5 epochs using a batch size of 8 since the data is really small for the example.\n",
    "# Let's start with the default learning rate of 0.01 and use a learning rate scheduler to lower the learning rate\n",
    "# as we progress.\n",
    "lr_scheduler = poutyne.StepLR(step_size=1, gamma=0.1)  # reduce LR by a factor of 10 each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98eaff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Train steps: 100 Val steps: 25 6.18s loss: 4.841451 accuracy: 85.520933 val_loss: 1.745821 val_accuracy: 91.753353\n",
      "Epoch 1: val_loss improved from inf to 1.74582, saving file to /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_1.ckpt\n",
      "Epoch: 2/5 Train steps: 100 Val steps: 25 6.15s loss: 1.477323 accuracy: 94.480761 val_loss: 1.559152 val_accuracy: 92.116948\n",
      "Epoch 2: val_loss improved from 1.74582 to 1.55915, saving file to /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_2.ckpt\n",
      "Epoch: 3/5 Train steps: 100 Val steps: 25 6.25s loss: 1.396164 accuracy: 94.783523 val_loss: 1.552267 val_accuracy: 92.205836\n",
      "Epoch 3: val_loss improved from 1.55915 to 1.55227, saving file to /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_3.ckpt\n",
      "Epoch: 4/5 Train steps: 100 Val steps: 25 6.30s loss: 1.251943 accuracy: 94.933769 val_loss: 1.551720 val_accuracy: 92.205836\n",
      "Epoch 4: val_loss improved from 1.55227 to 1.55172, saving file to /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_4.ckpt\n",
      "Epoch: 5/5 Train steps: 100 Val steps: 25 6.32s loss: 1.235884 accuracy: 95.181013 val_loss: 1.551642 val_accuracy: 92.205836\n",
      "Epoch 5: val_loss improved from 1.55172 to 1.55164, saving file to /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_5.ckpt\n",
      "Restoring data from /tf/empty_homes_data/deepparse_data/checkpoints21/checkpoint_epoch_5.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'time': 6.178594456003339,\n",
       "  'loss': 4.841450645063157,\n",
       "  'accuracy': 85.52093291700932,\n",
       "  'val_loss': 1.745821045935154,\n",
       "  'val_accuracy': 91.75335266113281},\n",
       " {'epoch': 2,\n",
       "  'time': 6.149312863999512,\n",
       "  'loss': 1.4773228324594951,\n",
       "  'accuracy': 94.48076105715339,\n",
       "  'val_loss': 1.5591523513197898,\n",
       "  'val_accuracy': 92.11694763183594},\n",
       " {'epoch': 3,\n",
       "  'time': 6.2536688369946205,\n",
       "  'loss': 1.3961643166933442,\n",
       "  'accuracy': 94.78352298593163,\n",
       "  'val_loss': 1.552267059981823,\n",
       "  'val_accuracy': 92.20583648681641},\n",
       " {'epoch': 4,\n",
       "  'time': 6.29782110099768,\n",
       "  'loss': 1.2519434146713793,\n",
       "  'accuracy': 94.93376857595037,\n",
       "  'val_loss': 1.5517196476459503,\n",
       "  'val_accuracy': 92.20583648681641},\n",
       " {'epoch': 5,\n",
       "  'time': 6.322195367007225,\n",
       "  'loss': 1.2358837545963757,\n",
       "  'accuracy': 95.18101277745755,\n",
       "  'val_loss': 1.5516423231363297,\n",
       "  'val_accuracy': 92.20583648681641}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The path to save our checkpoints\n",
    "logging_path = saving_dir +\"/checkpoints21\"\n",
    "\n",
    "address_parser.retrain(\n",
    "    training_container, 0.8, epochs=5, batch_size=8, num_workers=2, callbacks=[lr_scheduler], logging_path=logging_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0ad3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test\n",
      "Test steps: 1 0.19s test_loss: 1.189186 test_accuracy: 96.875000                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 0.187858019999112,\n",
       " 'test_loss': 1.1891863346099854,\n",
       " 'test_accuracy': 96.875}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's test our fine-tuned model using the best checkpoint (default parameter).\n",
    "address_parser.test(test_container, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e650f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepparse.dataset_container.dataset_container.PickleDatasetContainer at 0x7f49e7b52be0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_container\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
