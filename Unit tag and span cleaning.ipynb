{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3e22df",
   "metadata": {},
   "source": [
    "# Unit tag and Span cleaning\n",
    "\n",
    "This script combines two steps in the OCOD processing pipeline.\n",
    "\n",
    "* unit tagging\n",
    "* Removing overlapping spans\n",
    "\n",
    "These two processes are separated by the weak labelling in humanloop but as they are relatively simple they are included in a single script\n",
    "\n",
    "* **Raw CSV loaded and lightly processed. Output**: two column csv columns, property address, unit tag\n",
    "* Data labelled in programmatic. Output: json file of entities.\n",
    "* **Data programmatic output json cleaned ordered and overlaps removed**. Output: json file\n",
    "* Clean json converted to dataframe and multi-addresses expanded. Output: CSV\n",
    "* Count and locate addresses\n",
    "* Create address matcher and match businesses\n",
    "* Classify address types\n",
    "\n",
    "## Unit tagging\n",
    "\n",
    "This park of the pipeline adds in a binary value indicating whether the line contains flats/units/stores etc which are likely to have unit level ID. This is important as such addresses are likely to have a unit ID AND an street number and as such need to be treated with care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320c86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b9655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-407cae96ba14>:1: DtypeWarning: Columns (24,28,30,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ocod_data =  pd.read_csv('/tf/empty_homes_data/' +\n",
      "<ipython-input-6-407cae96ba14>:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
      "<ipython-input-6-407cae96ba14>:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n"
     ]
    }
   ],
   "source": [
    "ocod_data =  pd.read_csv('/tf/empty_homes_data/' +\n",
    "                    'OCOD_FULL_2022_02.csv',\n",
    "                   encoding_errors= 'ignore').rename(columns = lambda x: x.lower().replace(\" \", \"_\"))\n",
    "ocod_data['postcode'] = ocod_data['postcode'].str.upper()\n",
    "#empty addresses cannot be used. however there are only three so not a problem\n",
    "ocod_data = ocod_data.dropna(subset = 'property_address')\n",
    "ocod_data.reset_index(inplace = True, drop = True)\n",
    "ocod_data['property_address'] = ocod_data['property_address'].str.lower()\n",
    "\n",
    "#different words associated with unit ID's\n",
    "flatregex = r\"(flat|apartment|penthouse|unit)\" #unit|store|storage these a\n",
    "\n",
    "#This is not an exhaustive list of road names but it covers about 80% of all road types in the VOA business register.\n",
    "#The cardinal directions are includted as an option as they can appear after the road type. However they serve no real purpose in this particular regex and are \n",
    "#included for completness\n",
    "road_regex  = r\"((road|street|lane|way|gate|avenue|close|drive|hill|place|terrace|crescent|gardens|square|walk|grove|mews|row|view|boulevard|pleasant|vale|yard|chase|rise|green|passage|friars|viaduct|promenade|end|ridge|embankment|villas|circus))\\b( east| west| north| south)?\"\n",
    "#These names may be followed by a road type e.g. Earls court road. A negative lookahead is used to prevent these roads being tagged as units.\n",
    "flatregex2 = r\"(mansions|villa|court)\\b(?!(\\s\"+road_regex+\"))\"\n",
    "\n",
    "#flat_tag is used for legacy reasons but refers to sub-units in general\n",
    "ocod_data['flat_tag'] = ocod_data['property_address'].str.contains(flatregex + '|'+flatregex2, case = False)\n",
    "\n",
    "ocod_data['commercial_park_tag'] = ocod_data['property_address'].str.contains(r\"(retail|industrial|commercial|business|distribution|car)\", case = False)\n",
    "\n",
    "#typo in the data leads to a large number of fake flats\n",
    "ocod_data.loc[:, 'property_address'] = ocod_data['property_address'].str.replace(\"stanley court \", \"stanley court, \")\n",
    "\n",
    "#only two columns are needed for the humanloop labelling process\n",
    "ocod_data[['property_address', 'flat_tag', 'commercial_park_tag']].rename(columns = {'property_address':'text'}).to_csv('/tf/empty_homes_data/property_address_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503ca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the index for the ground truth\n",
    "random.seed(2017)\n",
    "test_set = random.sample([*range(0, ocod_data.shape[0])], 1000) \n",
    "\n",
    "test_set = ocod_data.loc[test_set, 'title_number'].reset_index().rename(columns = {'index':'datapoint_id'})\n",
    "\n",
    "test_set.to_csv('/tf/empty_homes_data/test_set_indices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd0d5d",
   "metadata": {},
   "source": [
    "## Labelling in Humanloop\n",
    "\n",
    "This part of process uses the humanloop programmatic app and is an external process. Once the labelling step is complete the process outputs a json file containing the labels and spans, this is then cleaned in the next step.\n",
    "\n",
    "## Removing overlapping spans\n",
    "\n",
    "During the humanloop tagging process the rules may result in the same words being tagged as part of multiple spans, this often occures for road names made up of multiple parts \n",
    "e.g. Canberra Crescent Gardens may be tagges as Canberra Cresecent and Canberra Crescent Gardens. The overlaps need to be removed before further prcoessing.\n",
    "For simplicity the largest span of any two overlapping spans is kept and the smaller of the two is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c6012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These libraries are specific to this part of the process\n",
    "import json\n",
    "import requests \n",
    "import config #contains hidden api key\n",
    "import operator #used for sorting the label dictionaries by start point. This is the basis for removing overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae4faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f =open(\"/tf/empty_homes_data/humanloop_02_04_22_t1400.json\")  #aggregate and download button\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "#this makes a list of all the observation rows. These refer to the row of the orginal observation text and so can be linked back to the original OCOD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c8de34",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-54d35ab49227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datapoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "data['datapoints']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint_id_list = [x['datapointId'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4270572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_and_labels = []\n",
    "data_labels_dict = []\n",
    "\n",
    "count_it = 0\n",
    "for i in set(datapoint_id_list):\n",
    "    count_it += 1\n",
    "    if count_it % 100 == 0: \n",
    "        print('count = {}'.format(count_it))\n",
    "        \n",
    "    single_id_index = np.where(np.array(datapoint_id_list)==i)\n",
    "    ##these labels are in tuple form\n",
    "   # list_of_labels = [(data[x]['start'], data[x]['end'], data[x]['label']) for x in single_id_index[0].tolist()]\n",
    "    \n",
    "    ##these labels are in dictionary form\n",
    "    list_of_labels_dict = [{'start': data[x]['start'], \n",
    "                            'end':data[x]['end'], \n",
    "                            'label': data[x]['label'], \n",
    "                            'label_text': data[x]['text'] } for x in single_id_index[0].tolist()]\n",
    "    \n",
    "    #this inplace sorting using operator orders the dictionary by the start point. ties are automatically broken\n",
    "    #it required the operator library\n",
    "    list_of_labels_dict.sort(key=operator.itemgetter('start'))\n",
    "    \n",
    "    list_of_labels_dict = remove_overlapping_spans2(list_of_labels_dict)\n",
    "\n",
    "    #create the NER dataset structure shown on the spacy website\n",
    "   # data_and_labels = data_and_labels + [ ( ocod_data['property_address'][i], list_of_labels ) ]\n",
    "    #create a list of dictionaries using a similar structure to save as a json\n",
    "    data_labels_dict = data_labels_dict + [\n",
    "        {\n",
    "            'text' : ocod_data['property_address'][i],\n",
    "            'labels' : list_of_labels_dict,\n",
    "            'datapoint_id': i,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('/tf/empty_homes_data/full_dataset_no_overlaps.json', 'w') as f:\n",
    "    json.dump(data_labels_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7798f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2e7f63",
   "metadata": {},
   "source": [
    "### Uploading to humanloop cloud\n",
    "\n",
    "This allows a sample of the data to be uploaded to the humanloop cloud so that an example model can be made.\n",
    "The model provides another way to check the quality of the weak labelling. However, only 10k obersvations can be uploaded, as such a sub-sample is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a random sub sample of data\n",
    "random.seed(10)\n",
    "data_labels_dict2 = random.sample(data_labels_dict, 9999)\n",
    "\n",
    "jason_test_data ={\n",
    "     \"name\": \"28_02_22_1010\",\n",
    "     \"description\": \"example labelling structure\",\n",
    "     \"fields\": [\n",
    "         {\"name\": \"text\", \n",
    "          \"data_type\": \"text\"\n",
    "         },\n",
    "         {\"name\": \"labels\", \n",
    "          \"data_type\": \"character_offsets\"},\n",
    "         {\"name\": \"datapoint_id\", \n",
    "          \"data_type\": \"text\"\n",
    "         }\n",
    "     ],\n",
    "     \"data\": data_labels_dict2\n",
    "}\n",
    "\n",
    "url = \"https://api.humanloop.com/datasets\"\n",
    "\n",
    "# replace payload with your actual dataset...\n",
    "payload= json.dumps(jason_test_data)\n",
    "headers = {\n",
    "  'X-API-Key': config.api_key,#the api key is hidden in a config file\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b7fab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41, 47, 'city'),\n",
       " (27, 39, 'street_name'),\n",
       " (48, 55, 'postcode'),\n",
       " (0, 25, 'building_name'),\n",
       " (27, 39, 'street_name')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list = data['datapoints'][0]['programmatic']['results']\n",
    "[(x['start'],x['end'],x['label'] ) for x in results_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f4bf2",
   "metadata": {},
   "source": [
    "## create spacy old format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce550f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3cc5decafd42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msingle_id_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m##these labels are in tuple form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlist_of_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_id_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3cc5decafd42>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msingle_id_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m##these labels are in tuple form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlist_of_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_id_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "datapoint_id_list = [*range(0, 100, 1)]\n",
    "data_and_labels = []\n",
    "data_labels_dict = []\n",
    "\n",
    "count_it = 0\n",
    "for i in set(datapoint_id_list):\n",
    "    count_it += 1\n",
    "    if count_it % 100 == 0: \n",
    "        print('count = {}'.format(count_it))\n",
    "        \n",
    "    single_id_index = np.where(np.array(datapoint_id_list)==i)\n",
    "    ##these labels are in tuple form\n",
    "    list_of_labels = [(data[x]['start'], data[x]['end'], data[x]['label']) for x in single_id_index[0].tolist()]\n",
    "    \n",
    "\n",
    "    #create the NER dataset structure shown on the spacy website\n",
    "    data_and_labels = data_and_labels + [ ( ocod_data['property_address'][i], list_of_labels ) ]\n",
    "\n",
    "    \n",
    "#Save the cleaned data back as a json file ready to be processed further  \n",
    "with open('/tf/empty_homes_data/humanloop_spacy_format.json', 'w') as f:\n",
    "    json.dump(data_labels_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
